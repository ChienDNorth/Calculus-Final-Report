%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{report}
\usepackage{xcolor}
\usepackage{sectsty}
\usepackage{framed}
\usepackage{ntheorem}
\usepackage{physics}%加適當長度的竪綫 \eval{}（evaluate）

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{MATH102 Calculus II \\1920 Final Report} % Title of the assignment
\author{Xue Ren Qianqian\\ \texttt{19098535-i011-0068}} % Author name and email address

\date{Macau University of Science and Technology \\\today} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\newtheorem{theorem}{THEOREM}
\newtheorem*{definition}{DEFINITION}
\newtheorem{case}{Case}

\begin{document}
\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	Chapter 7
%----------------------------------------------------------------------------------------
\setcounter{chapter}{6}
\chapter{Transcendental  Functions}
\section{Inverse Functions and Their Derivatives} % Numbered section
\begin{quote}
	
	%------------------------------------------------
	
	\subsection{One-to-One Functions}
	\begin{quote}

		A function that has distinct values at distinct elements in its domain is called one-to-one.
		
		\begin{quote}
			\subsubsection{The Horizontal Line Test for One-to-One Functions}
			
			A function y = ƒ(x) is one-to-one if and only if its graph intersects each horizontal line at most once.
		    
		\end{quote}
	\end{quote}
	
	%------------------------------------------------
	
	%------------------------------------------------
	
	\subsection{Inverse Functions}
	\begin{quote}
		
		%----------------------

		\begin{definition}

			Suppose that ƒ is a one-to-one function on a domain D with range R.  The inverse function $f^{-1}$ is defined by
			$$f^{-1}(b) = a \quad if \quad f(a) = b$$
			The domain of $f^{-1}$ is R and the range of $f^{-1}$ is D.
		
		\end{definition}

		%----------------------

		\begin{quote}


			\begin{warn}[Notice:]
				The symbol $f^{-1}$ for the inverse of ƒ is read “ƒ inverse.” The “-1” in $f^{-1}$ is not an exponent.
			\end{warn}
			
			\subsubsection{How to Find Inverses}
			\begin{enumerate}
				\item Solve the equation y = ƒ(x) for x. This gives a formula x = $f^{-1}(y)$ where x is expressed as a function of y.
				\item Interchange x and y, obtaining a formula y = $f^{-1}(x)$ where $f^{-1}$ is expressed in the conventional format with x as the independent variable and y as the dependent v ariable.
			\end{enumerate}
			
		\end{quote}
	\end{quote}
	
	%------------------------------------------------
	%	7.1.3
	%------------------------------------------------
	
	\subsection{Derivatives of Inverses of Differentiable Functions}
	\begin{quote}
		
		If y = ƒ(x) has a horizontal tangent line at (a, ƒ(a)), then the inverse function $f^{-1}$ has a vertical tangent line at (ƒ(a), a), and this infinite slope implies that $f^{-1}$ is not differentiable at ƒ(a). Theorem 1 gives the conditions under which $f^{-1}$ is differentiable in its domain (which is the same as the range of ƒ).
		
		\begin{quote}
			\begin{theorem}[The Derivative Rule for Inverses]
				\mbox{}\par %Go to next row

				If ƒ has an interval I as domain and $f'(x)$ exists and is never zero on I, then $f'(x)$ is diffrentiable at every point in its domain (the range of f). The value of $(f^{-1})'$ at a point b in the domain of $f^{-1}$ is the reciprocal of the value of $f'$ at the point $a = f^{-1}(b)$:

				$$(f^{-1})'(b)=\frac{1}{f'(f^{-1}(b))}$$

				or

				$$\eval{\frac{df^{-1}}{dx}}_{x=b} = \frac{1}{\eval{\frac{df}{dx}}}_{x = f^{-1}(b)}$$

			\end{theorem}
		\end{quote}

	\end{quote}
	
\end{quote}

%----------------------------------------------------------------------------------------
%	7.4 Exponential Change and Separable Differential Equations
%----------------------------------------------------------------------------------------
\setcounter{section}{3}

\section{Exponential Change and Separable Differential Equations} % Numbered section
\begin{quote}

	\subsection{Exponential Change}
	\begin{quote}

		If the amount present at time $t = 0$ is called $y_0$, then we can find y as a function of t by solving the following initial value problem:
	
		\begin{quote}


			Diferential equation: \qquad $\frac{dy}{dt}=ky$ \par
			\setlength{\parskip}{2pt}
			Initial condition:\qquad $y = y_{0}$ \quad when \quad $t = 0$.\par
			\setlength{\parskip}{0pt}

			\begin{info} % Information block
				If y is positive and increasing, then $k>0$.\\
				If y is positive and decreasing, then $k<0$
			\end{info}

			\subsubsection{The solution of the initial value problem}
			$$\frac{dy}{dt}=ky, \qquad y(0)=y_{0}$$
			is
			$$y=y_{0}e^{kt}.$$

		\end{quote}
		

	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	7.5 Exponential Change and Separable Differential Equations
%----------------------------------------------------------------------------------------

\section{Indeterminate Forms and L’Hôpital’s Rule}
\begin{quote}

	\subsection{Indeterminate Form $0/0$}
	\begin{quote}

		\begin{quote}
		\begin{theorem}
			Suppose that $f(a) = g(a) = 0$, that f and g are diffrentiable on an open interval I containing a, and that $g'(x) \neq 0$ on I if $x \neq a$. Then
			$$\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}$$
			assuming that the limit on the right side of this equation exists.
		\end{theorem}
		\end{quote}

		
		\subsubsection{Using L’Hôpital’s Rule }
		To find
		$$\lim_{x \to a} \frac{f(x)}{g(x)}$$
		by l’Hôpital’s Rule, we continue to differentiate ƒ and g, so long as we still get the form $0/0$ at x = a. But as soon as one or the other of these derivatives is different from zero at $x = a$ we stop differentiating. L’Hôpital’s Rule does not apply when either the numerator or denominator has a finite nonzero limit.
		
		\begin{info}
			More advanced treatments of calculus prove that l’Hôpital’s Rule applies to the indeterminate form $\infty / \infty$, as well as to $0/0$.
		\end{info}
		
	\end{quote}

	\subsection{Indeterminate Powers}
	\begin{quote}
		Limits that lead to the indeterminate forms $1^\infty, 0^0$, and $\infty^0$ can sometimes be handled by first taking the logarithm of the function.

		\begin{quote}
			If$\lim_{{x \to a }} \ln f(x) = L$, then
			$$\lim_{x \to a}f(x)= \lim_{x \to a} e ^{ln(f(x))} = e^L$$
			Here a may be either finite or infinite.
		\end{quote}

	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	7.6 Exponential Change and Separable Differential Equations
%----------------------------------------------------------------------------------------

\section{Inverse Trigonometric Functions}
\begin{quote}

	\subsection{Deining the Inverse Trigonometric Functions}
	\begin{quote}

		The six basic trigonometric functions are not one-to-one (since their values repeat periodically). However, we can restrict their domains to intervals on which they are one-to-one. Since these restricted functions are now one-to-one, they have inverses, which we denote by
		\begin{center}
			$y = \sin^{-1} x$ or $y = \arcsin x$,    $y = \cos^{-1} x$ or $y = \arccos x$\\
			$y = \tan^{-1} x$ or $y = \arctan x$,    $y = \cot^{-1} x$ or $y = \arccot x$\\
			$y = \sec^{-1} x$ or $y = \arcsec x$,    $y = \csc^{-1} x$ or $y = \arccsc x$
		\end{center}

	\end{quote}

	\subsection{The Arcsine and Arccosine Functions}
	\begin{quote}
		We define the arcsine and arccosine as functions whose values are angles (measured in radians) that belong to restricted domains of the sine and cosine functions.

		\begin{quote}
		\begin{definition}
			\mbox{}\par
				$\mathbf{y=\arcsin x}$ is the number in $[-\pi /2, \pi/2]$ for which $\sin y = x$\\
				$\mathbf{y=\arccos x}$ is the number in $[0,\pi]$for which $\cos y = x$

		\end{definition}
		\end{quote}

	\end{quote}

	\subsection{Identities Involving Arcsine and Arccosine	}
	\begin{quote}

		\begin{definition}
			$y = \arctan x$ is the number in $(-\pi/2,\pi/2)$ for which $\tan y = x$
			$y = \arccot x$ is the number in $(0,\pi)$ for which $\cot y = x$
			$y = \arcsec x$ is the number in $[0, \pi/2] \cup (\pi / 2,t]$ for which $\sec y = x$
			$y = \arccsc x$ is the number in $[-\pi/2,0) \cup (0,\pi/2]$ for which $\csc y = x$
		\end{definition}

	\end{quote}

	\subsection{The Derivative of the Inverse Trigonometric Functions}
	\begin{quote}

		\subsubsection{The Derivative of y = arcsin u}
		\begin{center}
			$\frac{d}{dx}(\arcsin u)= \frac{1}{\sqrt{1-u^2}} \frac{du}{dx}\qquad \left | u \right |<1$
		\end{center}

		\subsubsection{The Derivative of y = arctan u }
			$$\frac{d}{dx}(\arctan u) = \frac{1}{1+u^2} \frac{du}{dx}$$

		\subsubsection{Inverse Function–Inverse Cofunction Identities}
		\begin{quote}
			$$\arccos x = \pi/2 - \arcsin x$$
			$$\arccot x = \pi/2 - \arctan x$$
			$$\arccsc x = \pi/2 - \arcsec x$$
		\end{quote}
	
	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	Chapter 8
%----------------------------------------------------------------------------------------

\chapter{Transcendental  Functions}
\setcounter{section}{1}

%----------------------------------------------------------------------------------------
%	8.2
%----------------------------------------------------------------------------------------

\section{Integration by Parts}
\begin{quote}

	\subsection{Product Rule in Integral Form}
	\begin{quote}
		The \textbf{Product Rule} leading to these two formula.

		\begin{quote}
			\subsubsection{Integration by Parts Formula}
			$$\int u(x)\, v'(x) \,dx =u(x)\,v(x)-\int v(x)\,u'(x)\,dx$$

			\subsubsection{Integration by Parts Formula — Diferential Version}
			$$\int u\,{dv} = uv - \int v\,{du}$$

		\end{quote}

	\end{quote}

	\subsection{Evaluating Definite Integrals by Parts	}
	\begin{quote}

		The Fundamental Theorem gives 
		
		\begin{quote}
		\subsubsection{Integration by Parts Formula for Deﬁnite Integrals }
		$$\int_{a}{b}u(x)\,v'(x)\,dx = u(x)\,v(x) \bigg |_{a}^{b} - \int_{a}^{b}v(x)\, u'(x)\, dx $$
		\end{quote}

	\end{quote}
\end{quote}

%----------------------------------------------------------------------------------------
%	8.3
%----------------------------------------------------------------------------------------

\section{Trigonometric Integrals}
\begin{quote}

	\subsection{Products of Powers of Sines and Cosines}
	\begin{quote}

		We begin with integrals of the form 
		$$\int sin^m x\,cos^n x\, dx$$
		where m and n are nonnegative integers (positive or zero). We can divide the appropriate substitution into three cases according to m and n being odd or even.

		\begin{quote}

			\begin{case}
				If \textbf{m is odd}, we write m as 2k + 1 and use the identity $sin^2x = 1 - cos^2x$ to obtain 
				$$\sin^m x= \sin^{2k+1}x = (\sin^2x)^k \sin x = (1-\cos^2x)^k \sin x$$
				Then we combine the single $\sin x$ with dx in the integral and set $\sin x \,dx$ equal to $-d(\cos x)$.
			\end{case}

			\begin{case}
				If \textbf{n is odd} in $\int \sin^m x \cos^n x\, dx$, we write n as $2k + 1$ and use the i dentity $\cos^2x = 1 - \sin^2 x$ to obtain
				$$cos^nx=cos^{2k+1}x=(\cos^2x)^k \cos x =(1-\sin^2x)^k \cos x$$
				We then combine the single $\cos x$ with $dx$ and set $\cos x \, dx$ equal to $d(\sin x)$.
			\end{case}

			\begin{case}
				If \textbf{both m and n are even} in $\int \sin^m x \cos ^n x \, dx$,we substitute
				$$\sin^2x=\frac{1-\cos 2x}{2},\qquad \cos^2x = \frac{1+\cos 2x}{2}$$
				to reduce the integrand to one in lower powers of $\cos 2x$
			\end{case}

		\end{quote}

	\end{quote}

	\subsection{Integrals of Powers of $\tan x$ and $\sec x$}
	\begin{quote}

		We know how to integrate the tangent and secant functions and their squares. To integrate higher powers, we use the identities $\tan^2x = \sec^2 x - 1$ and $\sec^2 x = \tan^2 x + 1$, and integrate by parts when necessary to reduce the higher powers to lower powers.
		
	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	8.4
%----------------------------------------------------------------------------------------

\section{Trigonometric Substitutions}
\begin{quote}

	Trigonometric substitutions occur when we replace the variable of integration by a trigonometric function. The most common substitutions are $x = a \tan u$, $x = a \sin u$, and $x = a \sec u$. These substitutions are effective in transforming integrals involving $\sqrt{a^2+x^2}$ ,$\sqrt{a^2-x^2}$, $\sqrt{x^2-a^2}$ into integrals we can evaluate directly.
	\begin{table}[h]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		Substitutions & $x=a\tan \Theta$                                 & $x = a\sin \Theta$                             & $x = a \sec \Theta$                            \\ \hline
		Fit Forms     & $\sqrt{a^2+x^2} = a\left | \sec \Theta \right |$ & $\sqrt{a^2-x^2}=a\left | \cos \Theta \right |$ & $\sqrt{x^2-a^2}=a\left | \tan \Theta \right |$ \\ \hline
		\end{tabular}
	\end{table}

	\subsubsection{Procedure for a Trigonometric Substitution}
	\begin{enumerate}
		\item Write down the substitution for x, calculate the differential dx, and specify the selected values of u for the substitution.
		\item Substitute the trigonometric expression and the calculated diferential into the integrand, and then simplify the results algebraically.
		\item Integrate the trigonometric integral, keeping in mind the restrictions on the $\Theta$ for reversibility.
		\item Draw an appropriate reference triangle to reverse the substitution in the integration result and convert it back to the original variable x.
	\end{enumerate}

\end{quote}

%----------------------------------------------------------------------------------------
%	8.5
%----------------------------------------------------------------------------------------

\section{Integration of Rational Functions by Partial Fractions}
\begin{quote}

	The method for rewriting rational functions as a sum of simpler fractions is called the method of partial fractions.

	\subsection{General Description of the Method}
	\begin{quote}

		Success in writing a rational function ƒ(x)>g(x) as a sum of partial fractions depends on two things:

		\begin{itemize}
			\item The degree of ƒ(x) must be less than the degree of g(x). That is, the fraction must be proper. If it isn’t, divide ƒ(x) by g(x) and work with the remainder term. Example 3 of this section illustrates such a case.
			\item We must know the factors of g(x). In theory, any polynomial with real coeicients can be written as a product of real linear factors and real quadratic factors. In practice, the factors may be hard to find.
		\end{itemize}
		
		\subsubsection{Method of Partial Fractions When ƒ(x),g(x) Is Proper}
		\begin{enumerate}
			\item Let $x-r$ be a linear factor of g(x). Suppose that $(x-r)^m$ is the highest power of $x - r$ that divides g(x). Then, to this factor, assign the sum of the m partial fractions: $$\frac{A_1}{(x-r)}+\frac{A_2}{(x-r)^2}+\cdots+\frac{A_m}{(x-r)^m}$$ Do this for each distinct linear factor of g(x).
			\item Let $x^2 + px + q$ be an irreducible quadratic factor of g(x) so that $x^2 + px + q$ has no real roots. Suppose that $(x^2 + px + q)^n$ is the highest power of this factor that divides g(x). Then, to this factor, assign the sum of the n partial fractions: $$\frac{B_1x + C_1}{(x^2+px+q)}+\frac{B_2x+C_2}{(x^2+px+q)^2}+\cdots+\frac{B_nx+C_n}{(x^2+px+q)^n}.$$ Do this for each distinct quadratic factor of g(x).
			\item Set the original fraction $f(x)/g(x)$ equal to the sum of all these partial fractions. Clear the resulting equation of fractions and arrange the terms in decreasing powers of x.
			\item Equate the coefficients of corresponding powers of x and solve the resulting equations for the undetermined coefficients.
		\end{enumerate}
	
	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	8.8
%----------------------------------------------------------------------------------------

\setcounter{section}{7}
\section{Integration of Rational Functions by Partial Fractions}
\begin{quote}

	\subsection{Infinite Limits of Integration}
	\begin{quote}

		\begin{definition}

			Integrals with infinite limits of integration are \textbf{improper integrals of Type I}.
			
			\begin{enumerate}
				\item If f(x) is continuous on $\left [ a,\infty \right )$, then $$\int_{a}^{\infty}f(x)\,dx = \lim_{b \to \infty} \int_{a}^{b}f(x)\,dx.$$
				\item If f(x) is continuous on $\left( -\infty, b\right ]$, then $$\int_{-\infty}^{b} f(x)\, dx = \lim_{a \to -\infty}\int_{a}^{b}f(x)\,dx.$$
				\item If f(x) is continuous on $\left(-\infty, \infty \right )$, then $$\int_{-\infty}{\infty}f(x)\, dx = \int_{-\infty}^{c}f(x)\,dx+\int_{c}^{\infty}f(x)dx.$$ where c is any real number.
			\end{enumerate}

			In each case, if the limit exists and is finite, we say that the improper integral \textbf{converges} and that the limit is the \textbf{value} of the improper integral. If the limit fails to exist, the improper integral \textbf{diverges}.

		\end{definition}

	\end{quote}

	\subsection{The Integral $\int_{1}^{\infty}\frac{dx}{x^p}$}
	\begin{quote}

		The function $y = 1/x$ is the boundary between the convergent and divergent improper integrals with integrands of the form $y = 1/x^p$. As the following question shows, the improper integral \textbf{converges if} $\mathbf{p > 1}$ \textbf{and diverges if} $\mathbf{p \leqslant 1}$.

		%Here need a question
		
	\end{quote}

	\subsection{Integrands with Vertical Asymptotes}
	\begin{quote}
		
		\begin{definition}
			Integrals of functions that become infinite at a point within the interval f integration are \textbf{improper integrals of Type II}.
			\begin{enumerate}
				\item If f(x) is continuous on $\left (a,b\right ]$and discontinuous at a, then $$\int_{a}^{b}f(x)\,dx = \lim_{c\to a^+} \int_{c}^{b}f(x)\,dx$$
				\item If f(x) is continuous on $\left [a,b\right )$and discontinuous at b, then $$\int_{a}^{b}f(x)\,dx = \lim_{c \to b^-} \int_{a}^{c}f(x)\,dx$$
				\item If f(x) is discontinuous at c, where $a<c<b$,and continuous on $\left [a,c\right ) \cup \left(c,b\right ]$, then $$\int_{a}^{b}f(x)\,dx = \int_{a}^{c}f(x)\,dx + \int_{c}^{b}f(x)\,dx$$
			\end{enumerate}
		\end{definition}

	\end{quote}

	\subsection{Test for Convergence and Divergence}
	\begin{quote}
		When we cannot evaluate an improper integral directly, we try to determine whether it converges or diverges. If the integral diverges, that’s the end of the story. If it converges, we can use numerical methods to approximate its value. 
		\begin{quote}
			\begin{theorem}[Direct Comparison Test]
				Let f and g be continuous on $\left [ a,\infty \right )$ with $0\leq f(x)\leq g(x)$ for all$x\geq a$.Then
				\begin{enumerate}
					\item If $\int_{a}^{\infty} g(x)\, dx$converges, then $\int_{a}^{\infty}f(x)\,dx$ also vonverges.
					\item If $\int_{a}^{\infty} f(x)\, dx$diverges, then $\int_{a}^{\infty}g(x)\,dx$ also diverges.
				\end{enumerate}
			\end{theorem}

			\begin{theorem}
				If the positive functions f and g are continuous on $\left [a,\infty \right ]$, and If
				$$\lim_{x\to \infty}\frac{f(x)}{g(x)} = L, \qquad 0<L<\infty$$
				then
				$$\int_{a}^{\infty}f(x)\,dx\qquad and\qquad \int_{a}^{\infty}g(x)\,dx$$
				either both converge or both diverge.
			\end{theorem}

		\end{quote}
	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	Chapter 10
%----------------------------------------------------------------------------------------

\setcounter{chapter}{9}
\chapter{Infinite Sequences and Series }

%----------------------------------------------------------------------------------------
%	10.1
%----------------------------------------------------------------------------------------

\section{Sequences}
\begin{quote}

	\subsection{What is Sequences}
	\begin{quote}
		A sequence is a list of numbers in a given order.
	\end{quote}

	\subsection{Convergence and Divergence}
	\begin{quote}

		\begin{definition}
			The sequence $\left \{ an \right \}$ \textbf{converges} to the number L if for every positive number e there corresponds an integer N such that 
			$$\left | a_n - L\right | < \varepsilon \qquad whenever\qquad n > N$$
			If no such number L exists, we say that $\left \{ an \right \}$ \textbf{diverges}. 
			If $\left \{ an \right \}$ converges to L, we write $\lim_{n \to \infty}$, or simply $a_n \to L$, and call L the limit of the sequence.
		\end{definition}

		\begin{definition}
			The sequence $\left \{ an \right \}$ \textbf{diverges to infinity} if for every number M there is and integer N such that for all n larger than N, $a_n>M$. If this condition holds we write
			$$\lim_{n \to \infty} a_n = \infty \qquad or\qquad a_n \to \infty$$
			Similarly, if for every number m there is an integer N such that for all $n > N$ we have $a_n < m$, then we say $\left \{ an \right \}$ \textbf{diverges to negative infinity} and write 
			$$lim_{n \to \infty}a_n = -\infty\qquad or \qquad a_n\to\infty$$
		\end{definition}

		\begin{quote}
			\begin{theorem}[The Sandwich Theorem for Sequences]
				\mbox{}\par
				Let $\left \{ a_n \right \}$ $ \left\{ b_n \right \}$ and $ \left \{ c_n \right \}$ be sequences of real numbers. If an … bn … cn holds for all n beyond some index N, and if $\lim_{n\ to \infty} a_n = \lim_{n \to \infty} c_n = L$, then $\lim_{n \to \infty}b_n = L$also
			\end{theorem}
		\end{quote}


		\begin{quote}
			\begin{theorem}[The Continuous Function Theorem for Sequences]
				Let $\left\{a_n\right\}$ be a sequence of real numbers If $a_n \to L$ and if f is a function that is continuous at L and defined at all $a_n$, then $f(a_n)\to f(L)$
			\end{theorem}
		\end{quote}

	\end{quote}

	\subsection{Using L’Hôpital’s Rule}
	\begin{quote}

		We can also use  L’Hôpital’s Rule to find the limits of some sequences.

		\begin{quote}

			\begin{theorem}
				Suppose that f(x) is a function defined for al $x \geq n_0$ and that $\left\{a_n\right\}$ is a sequence of real numbers such that $a_n = f(x) $ for $n\geq n_0$. Then
				$$\lim_{n \to \infty a_n = L}\qquad whenever \qquad \lim_{x\to\infty} f(x) = L$$
			\end{theorem}
		
		\end{quote}

	\end{quote}

	\subsection{Commonly Occurring Limits}
	\begin{quote}

		The next theorem gives some limits that arise frequently.

		\begin{quote}
		\begin{theorem}
			\begin{enumerate}
				\item $\lim_{n \to \infty } \frac{\ln n}{n} = 0$
				\item $\lim_{n \to \infty }  \sqrt[n]{n} = 1$
				\item $\lim_{n \to \infty } x^{1/n}=1\qquad (x>0)$ 
				\item $\lim_{n \to \infty }  x^n = 0\qquad(\left | x \right | < 1)$
				\item $\lim_{n \to \infty } (1 + \frac{x}{n})^n = e^x\qquad (any\, x)$
				\item $\lim_{n \to \infty } \frac{x^n}{n!} = 0 \qquad (any\, x)$
			\end{enumerate}
		\end{theorem}
		\end{quote}

	\end{quote}

	\subsection{Bounded Monotonic Sequences}
	\begin{quote}

		\begin{definition}
			A sequence $\{an\}$ is \textbf{nondecreasing} if $a_n \leq a_{n+1}$ for all n. That is, $a_1 \leq a_2 \leq a_3 \leq \cdots$ The sequence is \textbf{nonincreasing} if $a_n \geq a_{n+1}$ for all n. The sequence $\{an\}$ is \textbf{monotonic} if it is either nondecreasing or nonincreasing.
		\end{definition}

		\begin{theorem}[The Monotonic Sequence Theorem]
			\mbox{}\par
			If a sequence $\{an\}$ is both bounded and monotonic, then the sequence converges. 
		\end{theorem}
			
	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	10.2
%----------------------------------------------------------------------------------------

\section{Infinite Series }
\begin{quote}

	\subsection{Geometric Series}
	\begin{quote}

		\textbf{Geometric series} are series of the form 
		$$a + ar + ar^2 +\cdots + ar^{n - 1} + \cdots = \sum_{n = 1}^{\infty} a r^{n-1}$$

		If $| r | < 1$, the geometric series $a + ar + ar^2 + \cdots + ar^{n-1} + \cdots$ converges to $a/(1-r)$:
		$$\int_{n = 1}^{\infty} ar^{n-1} = \frac{a}{1-r}\qquad |r|<1$$
		If $|r| \geq 1$, the series diverges.

	\end{quote}

	\subsection{The nth-Term Test for a Divergent Series}
	\begin{quote}

		\begin{theorem}
			If $\sum_{n=1}^{\infty}a_n$converges, then $a_n\to 0$
		\end{theorem}

		\subsubsection{The nth-Term Test for Divergence}
		$\sum_{n=1}^{\infty}a_n$diverges if $\lim_{n\to\infty}a_n$ fails to exist or is different from zero.

	\end{quote}

	\subsection{Combining Series}
	\begin{quote}

		These results is basic on Theorem.

		\begin{enumerate}
			\item Every nonzero constant multiple of a divergent series diverges.
			\item If$\sum a_n$converges and $\sum b_n$ diverges, then$\sum (a_n + b_n)$ and $\sum (a_n - b_n)$ both diverge.
		\end{enumerate}

	\end{quote}


\end{quote}

%----------------------------------------------------------------------------------------
%	10.3
%----------------------------------------------------------------------------------------

\section{The Integral Test }
\begin{quote}

	\subsection{Nondecreasing Partial Sums}
	\begin{quote}
		Since the partial sums form a nondecreasing sequence, the Monotonic Sequence Theorem (Section 10.1) gives the following result.
		\subsubsection{Corollary}
		A series $\sum \,_{n=1}^{\infty} a_n$ of nonnegative terms converges if and only if its partial sums are bounded from above.
	\end{quote}
	
	\subsection{The Integral Test}
	\begin{quote}
		\begin{theorem}
			Let $\{a_n\}$ be a sequence of positive terms. Suppose that $a_n = f(n)$, where f is a continuous, positive, decreasing function of x for all $x\geq N$(N a positive integer). Then the series $\sum_{n=N}^{\infty}a_n$and the integral $\int_{N}^{infty}f(x)\,dx$ both converge or bothe diverge.
		\end{theorem}

		\begin{info}[p-series]
			: $\sum_{n=1}^{\infty}\frac{1}{n^p}$ converges if $p>1$, diverges if $p\leq 1$
		\end{info}

	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	10.4
%----------------------------------------------------------------------------------------

\section{Comparison Tests }
\begin{quote}

	\begin{quote}
	\begin{theorem}[Direct Comparison Test]
		\mbox{}\par
		Let$\sum a_n$ and $\sum b_n $ be two series with $0\leq a_n \leq b_n$for all n. Then
		\begin{enumerate}
			\item If $\sum b_n$ converges, then $\sum a_n$ also converges.
			\item If $\sum a_n$ diverges, then $\sum b_n$ also diverges.
		\end{enumerate}
	\end{theorem}
	\end{quote}

	\subsection{The Limit Comparison Test}
	\begin{quote}
		
		\begin{quote}
			\begin{theorem}[Limit Comparison Test]
				Suppose that $a_n > 0$ and $b_n > 0$ for all $n \geq N$ (N an integer).
				\begin{enumerate}
					\item If $\lim_{n\to\infty} \frac{a_n}{b_n} = c$       and $c>0$, then $\sum a_n$and $\sum b_n$both converge of both diverge.
					\item If $\lim_{n\to\infty} \frac{a_n}{b_n} = 0$       and $\sum b_n$converges, then $\sum a_n$converges.
					\item If $\lim_{n\to\infty} \frac{a_n}{b_n} = \infty$  and $\sum b_n$diverges, then $\sum a_n$diverges.
				\end{enumerate}
			\end{theorem}
		\end{quote}

	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	10.5
%----------------------------------------------------------------------------------------

\section{Absolute Convergence; The Ratio and Root Tests }
\begin{quote}

		\subsection{Absolute Convergence}
		\begin{quote}

			\begin{definition}
				A series $\sum a_n$ \textbf{converges absolutely} (is \textbf{absolutely convergent})  if the corresponding series of absolute values, $\sum \left | a_n \right |$ , converges.
			\end{definition}

			\begin{quote}

				\begin{theorem}
					If $\sum_{n=1}^{\infty} \left | a_n \right | $ converges, then $ \sum_{n=1}^{\infty} a_n $ converges.
				\end{theorem}

			\end{quote}

		\end{quote}

	\subsection{The Ratio Test}
	\begin{quote}

		\begin{quote}
		\begin{theorem}
			Let $ \sum a_n $ be any series and suppose that
			$$\lim_{n \to \infty} \left | \frac{a_{n+1}}{a_n} \right | = \rho $$
			Then \textbf{(a)} the series \textbf{converges absolutely} if $\rho < 1$, \textbf{(b)} the series \textbf{diverges} if $ \rho > 0 $ or $\rho$ is ininite, \textbf{(c)} the test is \textbf{inconclusive} if $\rho = 1$
		\end{theorem}
		\end{quote}

	\end{quote}

	\subsection{The Root Test}
	\begin{quote}

		\begin{quote}
			\begin{theorem}

				Let $\sum a_n$ be any series and suppose that
				$$\lim_{n \to \infty } \sqrt[n]{\left | a_n \right |} = \rho$$
				Then \textbf{(a)} the series \textbf{converges absolutely} if $\rho < 1$, \textbf{(b)} the series \textbf{diverges} if $ \rho > 0 $ or $\rho$ is ininite, \textbf{(c)} the test is \textbf{inconclusive} if $\rho = 1$
			
			\end{theorem}
		\end{quote}

	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	10.6
%----------------------------------------------------------------------------------------

\section{Alternating Series and Conditional Convergence }
\begin{quote}

	A series in which the terms are alternately positive and negative is an alternating series. 

	\begin{quote}
		\begin{theorem}[The Alternating Series Test ]

			The series
			$$\sum_{n=1}^{\infty}(-1)^{n+1}u_n=u_1-u_2+u_3-u_4+\cdots$$
			converges if the following conditions are satisfied:

			\begin{enumerate}
				\item The $u_n$'s are all positive.
				\item The $u_n$ are eventually nonincreasing: $u_n \ge u_{n+1}$ for all $n\ge N$, for some integer N.
				\item $u_n \to 0$.
			\end{enumerate}

		\end{theorem}
	\end{quote}

	\begin{quote}
		\begin{theorem}[The Alternating Series Estimation Theorem]
			\mbox{}\par %Go to next row

			If the alternating series $\sum_{n=1}^{\infty} (-1)^{n+1} u_n$ satisies the three conditions of Theorem 15, then for $n \ge N$, 
			$$s_n = u_1 - u_2 \cdots + (-1)^{n+1}u_n$$
			approximates the sum L of the series with an error whose absolute value is less than $u_{n+1}$, the absolute value of the irst unused term. Furthermore, the sum L lies between any two successive partial sums $s_n$ and $s_{n+1}$, and the remainder, $L - s_n$, has the same sign as the irst unused term.

		\end{theorem}
	\end{quote}

	\subsection{Conditional Convergence}
	\begin{quote}
		
		\begin{definition}
		A series that is convergent but not absolutely convergent is called conditionally convergent.
		\end{definition}

	\end{quote}

	\subsection{Rearranging Series}
	\begin{quote}
		\begin{theorem}[The Rearrangement Theorem for Absolutely Convergent Series]
			\mbox{}\par

			If $\sum_{n=1}^{\infty}a_n$ converges absolutely, and $b_1, b_2, \cdots, b_n, \cdots$ is any arrangement of the sequence $\left \{ a_n \right \}$, then $\sum b_n$ converges absolutely and
			$$\sum_{n=1}^{\infty} b_n = \sum_{n=1}^{\infty}a_n$$

		\end{theorem}
	\end{quote}

	\subsection{Summary of Tests to Determine Convergence or Divergence}
	\begin{quote}

		Here is a summary of the tests we have considered.

		\begin{quote}
			
			\newpage
			
			\begin{enumerate}
				\item \textbf{The nth-term test for Divergence:} Unless $a_n \to 0$, the series diverges.
				\item \textbf{Geometric series:}                 $\sum ar^n$ converges if $\left | r \right |<1$; otherwise it diverges
				\item \textbf{p-series:}                         $\sum \frac{1}{n^p}$ converges if $p>1$; otherwise it diverges.
				\item \textbf{Series with nonnegative terms:}    Try the Integral Test or try comparing to a known series with the Direct Comparison Test or the Limit Comparison Test. Try the Ratio or Root Test.
				\item \textbf{Series with some negative terms:}  Does $\sum \left | a_n \right |$ converge by the Ratio or Root Test, or by another of the tests listed above? Remember, absolute convergence implies convergence.
				\item \textbf{Alternating series: }              $\sum a_n$ converges if the series satisfies the conditions of the Alternating Series Test.
			\end{enumerate}

			\begin{info} % Information block
				There are other tests we have not presented which are sometimes given in more advanced courses. 
			\end{info}
			
		\end{quote}


	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	10.7
%----------------------------------------------------------------------------------------

\section{Power Series }
\begin{quote}

	\subsection{Power Series and Convergence}
	\begin{quote}

		\begin{definition}
			\textbf{A power series about x = 0} is a series of the form
			$$\sum_{n=0}^{\infty} c_n x^n  = c_0 + c_1x + \cdots$$
			\textbf{A pover series about x = a} is a series of the form
			$$\sum_{n=0}^{\infty}c_n (x-a)^n = c_0 + c_1 (x-a) + c_2 (x-a)^2 + \cdots + c_n (x-a)^n + \cdots$$
			in which the \textbf{center} a and the \textbf{coefficients} $c_0, c_1, c_2,\cdots , c_n,\cdots$ are constants.
		\end{definition}

		\begin{quote}
			\begin{theorem}[The Convergence Theorem for Power Series ]
				\mbox{}\par
				If the power series
				$\sum_{n=0}^{\infty} a_n x^n = a_0 + a_1x + a_2x^2 + \cdots $ converges at $x = c \neq 0$, then it converges absolutely for all x with $\left | x \right | < \left | c \right |$. If the series diverges at $x = d$. then it diverges for all x with $ \left | x \right | > \left | d \right |$.
			\end{theorem}
		\end{quote}

	\end{quote}
\end{quote}

%----------------------------------------------------------------------------------------
%	10.8
%----------------------------------------------------------------------------------------

\section{Taylor and Maclaurin Series }
\begin{quote}

	\subsection{Taylor and Maclaurin Series	}
	\begin{quote}
		\begin{definition}
			Let ƒ be a function with derivatives of all orders throughout some interval containing a as an interior point. 
			Then the \textbf{Taylor series generated by f at} $\mathbf{x=a}$ is 
			$$\sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!}(x-a)^k = f(a) + f'(a)(x - a) + \frac{f''(a)}{2!} (x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n + \cdots$$
			The \textbf{Maclaurin series of f} is the Tailor series generated by f at $x=0$, or
			$$\sum_{k=0}^{\infty} \frac{f^{(k)}(0)}{k!} x^k = f(0)+ f'(0)x + \frac{f''(0)}{2!}x^2 + \cdots + f^{(n)}(0){n!}x^n + \cdots$$
		\end{definition}
	\end{quote}

	\subsection{Taylor Polynomials}
	\begin{quote}
		\begin{definition}
			Let ƒ be a function with derivatives of order k for $k = 1, 2, \cdots , N$ in some interval containing a as an interior point. Then for any integer n from 0 through N, the \textbf{Taylor polynomial of order n} generated by ƒ at $x = a$ is the polynomial
			$$P_n(x)=f(a) +f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)+\cdots + \frac{f^{(k)}(a)}{k!}(x - a)^k + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n$$
		\end{definition}
	\end{quote}

\end{quote} 

%----------------------------------------------------------------------------------------
%	Chapter 11
%----------------------------------------------------------------------------------------

\chapter{Parametric Equations and Polar Coordinates }

%----------------------------------------------------------------------------------------
%	11.1
%----------------------------------------------------------------------------------------

\section{Parametrizations of Plane Curves }
\begin{quote}
	
	\subsection{Parametric Equations}
	\begin{quote}

		Some path can describe as $x = f(t)$ and $y = g(t)$ where f and g are continuous functions.  When studying motion, t usually denotes time. Equations like these can describe more general curves than those described by a single function, and they provide not only the graph of the path traced out but also the location of the particle $(x, y) = (ƒ(t), g(t))$ at any time t.

	\end{quote}

	\subsection{Brachistochrones and Tautochrones}
	\begin{quote}

	\end{quote}


\end{quote}

%----------------------------------------------------------------------------------------
%	11.2
%----------------------------------------------------------------------------------------

\section{Calculus with Parametric Curves }
\begin{quote}

	\subsection{Tangents and Areas}
	\begin{quote}

		\begin{theorem}[Parametric Formula for dy/dx]
			\mbox{}\par
			If all three derivatives exist and $dx/dt \neq 0$, then
			$$\frac{dy}{dx}=\frac{dy/dt}{dx/dt}$$
		\end{theorem}
		
		\begin{theorem}[Parametric Formula for $d^2y/dx^2$]
			\mbox{}\par
			If the equations $x = ƒ(t), y = g(t)$ define y as a twice-differentiable function of x, then at any point where $dx/dt \neq 0$ and $y' = dy/dx$
			$$\frac{d^2y}{dx^2}= \frac{dy'/dt}{dx/dt}$$
		\end{theorem}
		
	\end{quote}

	\subsection{Length of a Parametrically Defined Curve}
	\begin{quote}
		\begin{definition}
			If a curve C is defined parametrically by $x = f(t)$ and $y = g(t)$, $a\leq t\leq b$, where f' and g' are continuous and not simultaneously zero on $\left [a,b\right ]$, and C is traversed exactly once as t increases from t = a to t = b, then \textbf{the length of C} is the definite integral
			$$L = \int_a^b \sqrt{[f'(t)]^2+[g'(t)]^2} \,dt$$
		\end{definition}

		\begin{info}
			If x = ƒ(t) and y = g(t), then using the Leibniz notation we can write the formula for arc length this way:
			$$L = \int_a^b \sqrt{\left (\frac{dx}{dt}\right )^2+\left (\frac{dy}{dt}\right )^2}\,dt$$
		\end{info}

	\end{quote}

	\subsection{The Arc Length Differential	}
	\begin{quote}
		
		The differential of arc length is
		$$ds = \sqrt{\left (\frac{dx}{dt}\right )^2 + \left(\frac{dy}{dt}\right )^2}\,dt$$
		and is often abbreviated as
		$$ds = \sqrt{dx^2+dy^2}$$

	\end{quote}

	\subsection{Areas of Surfaces of Revolution}
	\begin{quote}
		\subsubsection{Area of Surface of Revolution for Parametrized Curves}
		\begin{quote}

			If a smooth curve x = ƒ(t), y = g(t), $a\leq t\leq b$, is traversed exactly once as t increases from a to b, then the areas of the surfaces generated by revolving the curve about the coordinate axes are as follows.
			\begin{enumerate}
				\item {\textbf{Revolution about the x-axis}($\mathbf{y\geq 0}$):} $$S = \int_a^b 2\pi y \sqrt{\left (\frac{dx}{dt}\right )^2 + \left(\frac{dy}{dt}\right )^2}\,dt$$
				\item {\textbf{revolution about the y-axis}($\mathbf{x\geq 0}$):} $$S = \int_a^b 2\pi x \sqrt{\left (\frac{dx}{dt}\right )^2 + \left(\frac{dy}{dt}\right )^2}\,dt$$
			\end{enumerate}

		\end{quote}
	\end{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	11.3
%----------------------------------------------------------------------------------------

\section{Polar Coordinates }
\begin{quote}

	\subsection{Definition of Polar Coordinates	}
	\begin{quote}
		$$P(r,\theta)$$
		\textbf{Origin O}: called the \textbf{pole}\\
		\textbf{r}: Directed distance from O to P
		$\mathbf{\theta}$: Directed angle from initial ray to OP 
	\end{quote}

	\subsection{Relating Polar and Cartesian Coordinates}
	\begin{quote}

		\textbf{Equations Relating Polar and Cartesian Coordinates}\\
		$$x = r\cos \theta,\qquad y = r\sin \theta, \qquad r^2 = x^2 + y^2, \qquad \tan \theta =\frac{y}{x}$$

	\end{quote}


\end{quote}

%----------------------------------------------------------------------------------------
%	11.4
%----------------------------------------------------------------------------------------

\section{Graphing Polar Coordinate Equations }
\begin{quote}

\end{quote}

%----------------------------------------------------------------------------------------
%	11.5
%----------------------------------------------------------------------------------------

\section{Areas and Lengths in Polar Coordinates  }
\begin{quote}

\end{quote}

\end{document}
